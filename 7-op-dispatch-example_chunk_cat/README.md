# 0 python 与 c++ 的接口函数
- 相对路径: torch/csrc/autograd/generated/python_torch_functions_2.cpp
- 命名空间: torch::autograd
 
```python
// @generated from ../tools/autograd/templates/python_torch_functions.cpp

// Python bindings for torch.* functions implemented through ATen.
//
// The functions are bound as static methods on a class
// torch._C._VariableFunctions which is also aliased as Variable._torch
// and also copied into 'torch' module.

...

// _chunk_cat
static PyObject * THPVariable__chunk_cat(PyObject* self_, PyObject* args, PyObject* kwargs)
{
  HANDLE_TH_ERRORS
  static PythonArgParser parser({
    "_chunk_cat(TensorList tensors, int64_t dim, int64_t num_chunks, *, Tensor out=None)",
  }, /*traceable=*/true);

  ParsedArgs<4> parsed_args;
  auto _r = parser.parse(nullptr, args, kwargs, parsed_args);
  if(_r.has_torch_function()) {
    return handle_torch_function(_r, nullptr, args, kwargs, THPVariableFunctionsModule, "torch");
  }
  if (_r.isNone(3)) {
    // aten::_chunk_cat(Tensor[] tensors, int dim, int num_chunks) -> Tensor
    
    auto dispatch__chunk_cat = [](at::TensorList tensors, int64_t dim, int64_t num_chunks) -> at::Tensor {
      pybind11::gil_scoped_release no_gil;
      return at::_chunk_cat(tensors, dim, num_chunks);
    };
    return wrap(dispatch__chunk_cat(_r.tensorlist(0), _r.toInt64(1), _r.toInt64(2)));
  } else {
    // aten::_chunk_cat.out(Tensor[] tensors, int dim, int num_chunks, *, Tensor(a!) out) -> Tensor(a!)
    
    auto dispatch__chunk_cat_out = [](at::Tensor out, at::TensorList tensors, int64_t dim, int64_t num_chunks) -> at::Tensor {
      pybind11::gil_scoped_release no_gil;
      return at::_chunk_cat_out(out, tensors, dim, num_chunks);
    };
    return wrap(dispatch__chunk_cat_out(_r.tensor(3), _r.tensorlist(0), _r.toInt64(1), _r.toInt64(2)));
  }
  Py_RETURN_NONE;
  END_HANDLE_TH_ERRORS
}
```

# 1 at::_chunk_cat_out 接口
- build/aten/src/ATen/ops/_chunk_cat.h
- 命名空间：at

```python
#pragma once

// @generated by torchgen/gen.py from Function.h

#include <ATen/Context.h>
#include <ATen/DeviceGuard.h>
#include <ATen/TensorUtils.h>
#include <ATen/TracerMode.h>
#include <ATen/core/Generator.h>
#include <ATen/core/Reduction.h>
#include <ATen/core/Tensor.h>
#include <c10/core/Scalar.h>
#include <c10/core/Storage.h>
#include <c10/core/TensorOptions.h>
#include <c10/util/Deprecated.h>
#include <optional>



#include <ATen/ops/_chunk_cat_ops.h>

namespace at {


// aten::_chunk_cat(Tensor[] tensors, int dim, int num_chunks) -> Tensor
inline at::Tensor _chunk_cat(at::TensorList tensors, int64_t dim, int64_t num_chunks) {
    return at::_ops::_chunk_cat::call(tensors, dim, num_chunks);
}

// aten::_chunk_cat.out(Tensor[] tensors, int dim, int num_chunks, *, Tensor(a!) out) -> Tensor(a!)
inline at::Tensor & _chunk_cat_out(at::Tensor & out, at::TensorList tensors, int64_t dim, int64_t num_chunks) {
    return at::_ops::_chunk_cat_out::call(tensors, dim, num_chunks, out);
}
// aten::_chunk_cat.out(Tensor[] tensors, int dim, int num_chunks, *, Tensor(a!) out) -> Tensor(a!)
inline at::Tensor & _chunk_cat_outf(at::TensorList tensors, int64_t dim, int64_t num_chunks, at::Tensor & out) {
    return at::_ops::_chunk_cat_out::call(tensors, dim, num_chunks, out);
}

}
```

# 2 at::_ops算子struct类的声明

- torch/include/ATen/ops/_chunk_cat_ops.h
- 命名空间: at::_ops
  
```python
#pragma once

// @generated by torchgen/gen.py from Operator.h

#include <tuple>
#include <vector>

// Forward declarations of any types needed in the operator signatures.
// We can't directly include these classes because it will cause circular include dependencies.
// This file is included by TensorBody.h, which defines the Tensor class.
#include <ATen/core/ATen_fwd.h>

namespace at {
namespace _ops {


struct TORCH_API _chunk_cat {
  using schema = at::Tensor (at::TensorList, int64_t, int64_t);
  using ptr_schema = schema*;
  // See Note [static constexpr char* members for windows NVCC]
  STATIC_CONSTEXPR_STR_INL_EXCEPT_WIN_CUDA(name, "aten::_chunk_cat")
  STATIC_CONSTEXPR_STR_INL_EXCEPT_WIN_CUDA(overload_name, "")
  STATIC_CONSTEXPR_STR_INL_EXCEPT_WIN_CUDA(schema_str, "_chunk_cat(Tensor[] tensors, int dim, int num_chunks) -> Tensor")
  static at::Tensor call(at::TensorList tensors, int64_t dim, int64_t num_chunks);
  static at::Tensor redispatch(c10::DispatchKeySet dispatchKeySet, at::TensorList tensors, int64_t dim, int64_t num_chunks);
};

struct TORCH_API _chunk_cat_out {
  using schema = at::Tensor & (at::TensorList, int64_t, int64_t, at::Tensor &);
  using ptr_schema = schema*;
  // See Note [static constexpr char* members for windows NVCC]
  STATIC_CONSTEXPR_STR_INL_EXCEPT_WIN_CUDA(name, "aten::_chunk_cat")
  STATIC_CONSTEXPR_STR_INL_EXCEPT_WIN_CUDA(overload_name, "out")
  STATIC_CONSTEXPR_STR_INL_EXCEPT_WIN_CUDA(schema_str, "_chunk_cat.out(Tensor[] tensors, int dim, int num_chunks, *, Tensor(a!) out) -> Tensor(a!)")
  static at::Tensor & call(at::TensorList tensors, int64_t dim, int64_t num_chunks, at::Tensor & out);
  static at::Tensor & redispatch(c10::DispatchKeySet dispatchKeySet, at::TensorList tensors, int64_t dim, int64_t num_chunks, at::Tensor & out);
};

}} // namespace at::_ops
```

# 3. at::_ops算子struct 静态方法的实现（call 和 redispatch）
- build/aten/src/ATen/Operators_4.cpp
- 命名空间:at::_ops
  
```c++
#include <ATen/Tensor.h>
#include <ATen/core/dispatch/Dispatcher.h>

// @generated by torchgen/gen.py from Operators.cpp
// NOTE See [Sharded File] comment in VariableType

#ifndef AT_PER_OPERATOR_HEADERS
#include <ATen/Operators.h>

...

// aten::_chunk_cat.out(Tensor[] tensors, int dim, int num_chunks, *, Tensor(a!) out) -> Tensor(a!)
static C10_NOINLINE c10::TypedOperatorHandle<_chunk_cat_out::schema> create__chunk_cat_out_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(_chunk_cat_out::name, _chunk_cat_out::overload_name)
      .typed<_chunk_cat_out::schema>();
}

// aten::_chunk_cat.out(Tensor[] tensors, int dim, int num_chunks, *, Tensor(a!) out) -> Tensor(a!)
at::Tensor & _chunk_cat_out::call(at::TensorList tensors, int64_t dim, int64_t num_chunks, at::Tensor & out) {
    
    static auto op = create__chunk_cat_out_typed_handle(); // 在此会调用dispatch 机制
    return op.call(tensors, dim, num_chunks, out);
}

// aten::_chunk_cat.out(Tensor[] tensors, int dim, int num_chunks, *, Tensor(a!) out) -> Tensor(a!)
at::Tensor & _chunk_cat_out::redispatch(c10::DispatchKeySet dispatchKeySet, at::TensorList tensors, int64_t dim, int64_t num_chunks, at::Tensor & out) {
    
    static auto op = create__chunk_cat_out_typed_handle();
    return op.redispatch(dispatchKeySet, tensors, dim, num_chunks, out); // 在此会调用redispatch机制
}
```

**遗留问题：call 和 redispatch 有何不同 ???** <br>

# 4 中间diapatch流程
- c10::TypedOperatorHandle::call at /home/mtn_torch/pytorch/aten/src/ATen/core/dispatch/Dispatcher.h:531
- c10::Dispatcher::call          at /home/mtn_torch/pytorch/aten/src/ATen/core/dispatch/Dispatcher.h:698
- c10::KernelFunction::call      at /home/mtn_torch/pytorch/aten/src/ATen/core/boxing/KernelFunction_impl.h:109
- c10::impl::BoxedKernelWrapper::call at /home/mtn_torch/pytorch/aten/src/ATen/core/boxing/impl/boxing.h:336
- c10::BoxedKernel::callBoxed         at /home/mtn_torch/pytorch/aten/src/ATen/core/boxing/BoxedKernel_impl.h:41
- c10::BoxedKernel::make_boxed_function at /home/mtn_torch/pytorch/torch/csrc/autograd/autograd_not_implemented_fallback.cpp:475
- torch::autograd::autogradNotImplementedFallbackImpl at /home/mtn_torch/pytorch/torch/csrc/autograd/autograd_not_implemented_fallback.cpp:356
- c10::OperatorHandle::redispatchBoxed  at /home/mtn_torch/pytorch/aten/src/ATen/core/dispatch/Dispatcher.h:473
- c10::Dispather::redispatchBoxed       at /home/mtn_torch/pytorch/aten/src/ATen/core/dispatch/Dispatcher.h:779
- c10::KernelFunction::callBoxed        at /home/mtn_torch/pytorch/aten/src/ATen/core/boxing/KernelFunction_impl.h:46
- c10::BoxedKernel::callBoxed           at /home/mtn_torch/pytorch/aten/src/ATen/core/boxing/BoxedKernel_impl.h:41
- c10::impl::make_boxed_from_unboxed_functor::call at /home/mtn_torch/pytorch/aten/src/ATen/core/boxing/impl/make_boxed_from_unboxed_functor.h:584
- c10::impl::call_functor_with_args_from_stack     at /home/mtn_torch/pytorch/aten/src/ATen/core/boxing/impl/make_boxed_from_unboxed_functor.h:518
- c10::impl::call_functor_with_args_from_stack     at /home/mtn_torch/pytorch/aten/src/ATen/core/boxing/impl/make_boxed_from_unboxed_functor.h:503
- c10::impl::wrap_kernel_functor_unboxed::call     at /home/mtn_torch/pytorch/aten/src/ATen/core/boxing/impl/make_boxed_from_unboxed_functor.h:485
- c10::impl::detail::WrapFuntionIntoFunctor::operator() at /home/mtn_torch/pytorch/aten/src/ATen/core/boxing/impl/WrapFunctionIntoFunctor.h

# 5 分发到ADInplaceOrView
- torch/csrc/autograd/generated/ADInplaceOrViewType_1.cpp : 731
- 命名空间: torch::ADInplaceView 

```c++
#define TORCH_ASSERT_ONLY_METHOD_OPERATORS
#include "torch/csrc/autograd/VariableTypeUtils.h"
#include "torch/csrc/autograd/generated/ViewFuncs.h"

#include <torch/library.h>
#include <ATen/FunctionalInverses.h>
#include <ATen/FunctionalTensorWrapper.h>

// @generated from ../tools/autograd/templates/ADInplaceOrViewType.cpp

...
namespace torch {
namespace ADInplaceOrView {
namespace {

// 此函数在匿名空间
at::Tensor & _chunk_cat_out_out(c10::DispatchKeySet ks, at::TensorList tensors, int64_t dim, int64_t num_chunks, at::Tensor & out) {
  {
    at::AutoDispatchBelowADInplaceOrView guard;
    at::_ops::_chunk_cat_out::redispatch(ks & c10::after_ADInplaceOrView_keyset, tensors, dim, num_chunks, out);
  }
  increment_version(out);
  return out;
}

} // namespace anonymous
} // namespace ADInplaceOrView
} // namespace torch

TORCH_LIBRARY_IMPL(aten, ADInplaceOrView, m) {
  m.impl("_chunk_cat.out",
         TORCH_FN(ADInplaceOrView::_chunk_cat_out_out)
  );
```

# 6 再次调度到Operators
 - build/aten/src/ATen/Operators_4.cpp
 - 命名空间:at::_ops

```c++
#include <ATen/Tensor.h>
#include <ATen/core/dispatch/Dispatcher.h>

// @generated by torchgen/gen.py from Operators.cpp
// NOTE See [Sharded File] comment in VariableType

#ifndef AT_PER_OPERATOR_HEADERS
#include <ATen/Operators.h>

...

// aten::_chunk_cat.out(Tensor[] tensors, int dim, int num_chunks, *, Tensor(a!) out) -> Tensor(a!)
static C10_NOINLINE c10::TypedOperatorHandle<_chunk_cat_out::schema> create__chunk_cat_out_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(_chunk_cat_out::name, _chunk_cat_out::overload_name)
      .typed<_chunk_cat_out::schema>();
}

// aten::_chunk_cat.out(Tensor[] tensors, int dim, int num_chunks, *, Tensor(a!) out) -> Tensor(a!)
at::Tensor & _chunk_cat_out::call(at::TensorList tensors, int64_t dim, int64_t num_chunks, at::Tensor & out) {
    
    static auto op = create__chunk_cat_out_typed_handle(); // 在此会调用dispatch 机制
    return op.call(tensors, dim, num_chunks, out);
}

// aten::_chunk_cat.out(Tensor[] tensors, int dim, int num_chunks, *, Tensor(a!) out) -> Tensor(a!)
at::Tensor & _chunk_cat_out::redispatch(c10::DispatchKeySet dispatchKeySet, at::TensorList tensors, int64_t dim, int64_t num_chunks, at::Tensor & out) {
    
    static auto op = create__chunk_cat_out_typed_handle();
    return op.redispatch(dispatchKeySet, tensors, dim, num_chunks, out); // 在此会调用redispatch机制
}
```

# 7 再次进行dispatch流程
- c10::TypedOperatorHandle::redispatch  at /home/mtn_torch/pytorch/aten/src/ATen/core/dispatch/Dispatcher.h:536
- c10::Dispatcher::redispatch           at /home/mtn_torch/pytorch/aten/src/ATen/core/dispatch/Dispatcher.h:714
- c10::KernelFunction::call             at /home/mtn_torch/pytorch/aten/src/ATen/core/boxing/KernelFunction_impl.h:104
- c10::callUnboxedKernelFunction::call  at /home/mtn_torch/pytorch/aten/src/ATen/core/boxing/KernelFunction_impl.h:53
- c10::impl::wrap_kernel_functor_unboxed::call at /home/mtn_torch/pytorch/aten/src/ATen/core/boxing/impl/make_boxed_from_unboxed_functor.h:468
- c10::impl::detail::WrapFunctionIntoFunctor::operator() at /home/mtn_torch/pytorch/aten/src/ATen/core/boxing/impl/WrapFunctionIntoFunctor.h:13

# 8 redispatch : 走进CompositeExplicet warp op
- ~/build/aten/src/ATen/RegisterCompositeExplicitAutograd.cpp:3982
- 命名空间: compositeexplicitautograd

```c++
// @generated by torchgen/gen.py from RegisterDispatchKey.cpp

namespace at {
namespace {
at::Tensor & wrapper_CompositeExplicitAutograd_out__chunk_cat_out(at::TensorList tensors, int64_t dim, int64_t num_chunks, at::Tensor & out) {
    // No device check
  // DeviceGuard omitted
  return at::native::_chunk_cat_out(tensors, dim, num_chunks, out);
}
} // anonymous namespace

// 在此被注册
TORCH_LIBRARY_IMPL(aten, CompositeExplicitAutograd, m) {
  m.impl("_chunk_cat",
  TORCH_FN(wrapper_CompositeExplicitAutograd___chunk_cat));
  m.impl("_chunk_cat.out",
  TORCH_FN(wrapper_CompositeExplicitAutograd_out__chunk_cat_out));
}

// 同时将这个个explictAutograd函数的作为单独的api
namespace compositeexplicitautograd {
  at::Tensor & _chunk_cat_out(at::Tensor & out, at::TensorList tensors, int64_t dim, int64_t num_chunks) {
  return wrapper_CompositeExplicitAutograd_out__chunk_cat_out(tensors, dim, num_chunks, out);
  }
  at::Tensor & _chunk_cat_outf(at::TensorList tensors, int64_t dim, int64_t num_chunks, at::Tensor & out) {
  return wrapper_CompositeExplicitAutograd_out__chunk_cat_out(tensors, dim, num_chunks, out);
  }
} // namespace compositeexplicitautograd
} // namespace at
```

# 9 at::native 下函数
- aten/src/ATen/native/TensorShape.cpp:2754
- 命名空间：at::native

```c++
namespace at::native {
Tensor& _chunk_cat_out(TensorList tensors, int64_t dim, int64_t num_chunks, Tensor& out) {
  auto wrapped_dim = at::native::preprocess_chunk_cat_inputs(tensors, dim, num_chunks);
  at::cat_out(out, _pad_chunk(tensors, wrapped_dim, num_chunks), wrapped_dim+1);
  return out;
}
} // namespece at::native
```

# 10 at 下另一函数的调用
- build/aten/src/ATen/ops/cat.h : 统一的函数api
- 命名空间at

```c++
#pragma once

// @generated by torchgen/gen.py from Function.h

#include <ATen/Context.h>
#include <ATen/DeviceGuard.h>
#include <ATen/TensorUtils.h>
#include <ATen/TracerMode.h>
#include <ATen/core/Generator.h>
#include <ATen/core/Reduction.h>
#include <ATen/core/Tensor.h>
#include <c10/core/Scalar.h>
#include <c10/core/Storage.h>
#include <c10/core/TensorOptions.h>
#include <c10/util/Deprecated.h>
#include <optional>



#include <ATen/ops/cat_ops.h>

namespace at {


// aten::cat(Tensor[] tensors, int dim=0) -> Tensor
inline at::Tensor cat(const at::ITensorListRef & tensors, int64_t dim=0) {
    return at::_ops::cat::call(tensors, dim);
}

// aten::cat.out(Tensor[] tensors, int dim=0, *, Tensor(a!) out) -> Tensor(a!)
inline at::Tensor & cat_out(at::Tensor & out, const at::ITensorListRef & tensors, int64_t dim=0) {
    return at::_ops::cat_out::call(tensors, dim, out);
}
// aten::cat.out(Tensor[] tensors, int dim=0, *, Tensor(a!) out) -> Tensor(a!)
inline at::Tensor & cat_outf(const at::ITensorListRef & tensors, int64_t dim, at::Tensor & out) {
    return at::_ops::cat_out::call(tensors, dim, out);
}

// aten::cat.names(Tensor[] tensors, Dimname dim) -> Tensor
inline at::Tensor cat(at::TensorList tensors, at::Dimname dim) {
    return at::_ops::cat_names::call(tensors, dim);
}

// aten::cat.names_out(Tensor[] tensors, Dimname dim, *, Tensor(a!) out) -> Tensor(a!)
inline at::Tensor & cat_out(at::Tensor & out, at::TensorList tensors, at::Dimname dim) {
    return at::_ops::cat_names_out::call(tensors, dim, out);
}
// aten::cat.names_out(Tensor[] tensors, Dimname dim, *, Tensor(a!) out) -> Tensor(a!)
inline at::Tensor & cat_outf(at::TensorList tensors, at::Dimname dim, at::Tensor & out) {
    return at::_ops::cat_names_out::call(tensors, dim, out);
}

}
```

## 11 到 cat 的struct下以进行dispatch
- build/aten/src/ATen/Operators_0.cpp
- 命名空间: at::_ops

```c++
// aten::cat.out(Tensor[] tensors, int dim=0, *, Tensor(a!) out) -> Tensor(a!)
static C10_NOINLINE c10::TypedOperatorHandle<cat_out::schema> create_cat_out_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(cat_out::name, cat_out::overload_name)
      .typed<cat_out::schema>();
}

// aten::cat.out(Tensor[] tensors, int dim=0, *, Tensor(a!) out) -> Tensor(a!)
at::Tensor & cat_out::call(const at::ITensorListRef & tensors, int64_t dim, at::Tensor & out) {
    
    static auto op = create_cat_out_typed_handle();
    return op.call(tensors, dim, out);
}

// aten::cat.out(Tensor[] tensors, int dim=0, *, Tensor(a!) out) -> Tensor(a!)
at::Tensor & cat_out::redispatch(c10::DispatchKeySet dispatchKeySet, const at::ITensorListRef & tensors, int64_t dim, at::Tensor & out) {
    
    static auto op = create_cat_out_typed_handle();
    return op.redispatch(dispatchKeySet, tensors, dim, out);
}
```

## 12 进行dispatch
- c10::TypedOperatorHandle::call at /home/mtn_torch/pytorch/aten/src/ATen/core/dispatch/Dispatcher.h:531
- c10::Dispatcher::call          at /home/mtn_torch/pytorch/aten/src/ATen/core/dispatch/Dispatcher.h:698
- c10::KernelFunction::call      at /home/mtn_torch/pytorch/aten/src/ATen/core/boxing/KernelFunction_impl.h:104
- c10::callUnboxedKernelFunction::call at /home/mtn_torch/pytorch/aten/src/ATen/core/boxing/KernelFunction_impl.h:53
- c10::wrap_kernel_functor_unboxed::call        at /root/mtn/FSDP2/torch_musa/build/generated_cuda_compatible/include/ATen/core/boxing/impl/make_boxed_from_unboxed_functor.h:368
- c10::impl::detail::WrapFunctionIntoRuntimeFunctor_::operator() at /root/mtn/FSDP2/torch_musa/build/generated_cuda_compatible/include/ATen/core/boxing/impl/WrapFunctionIntoRuntimeFunctor.h:18

# 13 musa wrapper 函数
- /root/mtn/FSDP2/torch_musa/build/torch_musa_codegen/ATen/RegisterMUSA.cpp
- at::(anonymous)

```
// @generated by torch_musa/codegen/gen_torch_musa.py from RegisterMUSA.cpp

namespace at {

namespace {
at::Tensor wrapper_PrivateUse1__cat(const at::ITensorListRef & tensors, int64_t dim) {
    // No device check
  // DeviceGuard omitted
  return at::musa::Cat(tensors, dim);
}
} // anonymous namespace
namespace {
at::Tensor & wrapper_PrivateUse1_out_cat_out(const at::ITensorListRef & tensors, int64_t dim, at::Tensor & out) {
    // No device check
  // DeviceGuard omitted
  return at::musa::CatOut(tensors, dim, out);
}
} // anonymous namespace

ADVANCED_REGISTER(aten, PrivateUse1, "cat", wrapper_PrivateUse1__cat)
ADVANCED_REGISTER(aten, PrivateUse1, "cat.out", wrapper_PrivateUse1_out_cat_out)

// 同时暴露出单独的调用接口
namespace musa {
at::Tensor cat(const at::ITensorListRef & tensors, int64_t dim) {
return wrapper_PrivateUse1__cat(tensors, dim);
}
at::Tensor & cat_out(at::Tensor & out, const at::ITensorListRef & tensors, int64_t dim) {
return wrapper_PrivateUse1_out_cat_out(tensors, dim, out);
}
at::Tensor & cat_outf(const at::ITensorListRef & tensors, int64_t dim, at::Tensor & out) {
return wrapper_PrivateUse1_out_cat_out(tensors, dim, out);
}
} // namespace musa
} // namespace at

```

# 14 最终musa实现函数
- /root/mtn/FSDP2/torch_musa/torch_musa/csrc/aten/ops/Concat.cpp
- 命名空间: at::musa
```c++
namespace at::musa {
Tensor& CatOut(const at::ITensorListRef& tensors, int64_t dim, Tensor& out) {
  ...
}
} namespace at::musa
```
